{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Smart Pixels (ML)","text":"<p>Smart Pixels is a project focused on implementing machine learning models directly on silicon pixels or future detectors to enhance the inference of charged particle track parameters. We here use an MDN (Mixture Density Network) with one Gaussian. This model predicts the mean value of the target variable and the associated uncertainty to that prediction, with NLL (Negative Log-likelihood Loss) as the loss function to minimize. </p>"},{"location":"index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Installation</li> <li>Usage</li> <li>Dataset</li> <li>Structure</li> <li>Data Visualization</li> <li>Data Generator</li> <li>Testing</li> <li>Model Architecture</li> <li>Model Evaluation</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":""},{"location":"index.html#installation","title":"Installation","text":"<ol> <li>Clone the repository: <pre><code>git clone https://github.com/smart-pix/smart-pixels-ml.git\ncd smart-pixels-ml\npip install -r requirements.txt\n</code></pre> Testing successful installation by running  <pre><code>python test_run.py`\n</code></pre> The test run should perform a basic smoke test to check if the installation is successful and basic things that we need are working fine. Details can be found in Testing <p>The output should look like this:</p> </li> </ol> <p></p>"},{"location":"index.html#usage","title":"Usage","text":"<p>For the usage guide refer to Usage Guide</p>"},{"location":"index.html#dataset","title":"Dataset","text":"<p>Download the simulated data from: zenodo and PixelAV</p>"},{"location":"index.html#structure","title":"Structure","text":"<ul> <li> <p>Input Data (dataX): Consists of 2D images representing the charge deposited by a particle, with each channel showing a different time slice of the particle\u2019s passage. You can choose to work with either <code>2</code> time slices (reduced) or the full <code>20</code> time slices. These correspond to input shapes of <code>(13, 21, 2)</code> and <code>(13, 21, 20)</code> respectively.</p> </li> <li> <p>Labels (dataY): Four target variables are chosen as the labls viz. local $x$, local $y$, $\\alpha$ angle and $\\beta$ angle, associated with the particle trajectory.</p> </li> </ul>"},{"location":"index.html#data-visualization","title":"Data Visualization","text":"<p>For visualization of the how the input data looks like, we have to define the path towards the <code>dataX</code> and optionally to the labels as  <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndataX = pd.read_parquet(\"path/to/recon3D_data_file\")\nlabels_df = pd.read_parquet(\"path/to/labels_data_file\")\nreshaped_dataX = dataX.values.reshape((len(dataX), 20, 13, 21))\n\nprint(labels_df.iloc[0])\nplt.imshow(reshaped_dataX[0, 0, :, :], cmap='coolwarm')  # first time-step\nplt.show()\nplt.imshow(reshaped_dataX[0, -1, :, :], cmap='coolwarm')  # last time-step\nplt.show()\n</code></pre></p>"},{"location":"index.html#data-generator","title":"Data Generator","text":"<p>Due to the large size of the dataset, the entire dataset can not be loaded into RAM. Hence, we use data generators to load the dataset on the fly during training with inbuilt quantization, standardization, shuffling, etc.  Refer to data generator for more details.</p>"},{"location":"index.html#testing","title":"Testing","text":"<p>To test that everything is working fine try running the simple <code>test_run.py</code> file as <pre><code>python test_run.py\n</code></pre></p>"},{"location":"index.html#model-architecture","title":"Model Architecture","text":"<p>The core model architecture is defined in model.py, which provides the baseline MDN architecture with quantized neural network layers. We use the Negative Log-Likelihood (NLL) as the loss function implemented in loss.py. A good reading about it can be found here</p> <p>Refer to Model and Loss for more details.</p> <p></p> <p>As an example, to implement the model with 2 time slices: <pre><code>from model import *\n\nmodel=CreateModel((13,21,2),n_filters=5,pool_size=3)\nmodel.summary()\n</code></pre> This generates a model with the following architecture:</p> Type Output Shape Parameters InputLayer (None, 13, 21, 2) 0 QSeparableConv2D (None, 11, 19, 5) 33 QActivation (None, 11, 19, 5) 0 QConv2D (None, 11, 19, 5) 30 QActivation (None, 11, 19, 5) 0 AveragePooling2D (None, 3, 6, 5) 0 QActivation (None, 3, 6, 5) 0 Flatten (None, 90) 0 QDense (None, 16) 1456 QActivation (None, 16) 0 QDense (None, 16) 272 QActivation (None, 16) 0 QDense (None, 14) 238"},{"location":"index.html#model-evaluation","title":"Model Evaluation","text":"<p>Refer to Evaluate for more details</p>"},{"location":"plot.html","title":"Plots","text":"<p>Add things about the plots here</p>"},{"location":"testing.html","title":"Testing Module","text":"<p>This module contains unit tests for the Smart Pixels ML project.</p>"},{"location":"testing.html#scripts","title":"Scripts","text":""},{"location":"testing.html#test_runpy","title":"<code>test_run().py</code>","text":"<ul> <li> <p>Description: </p> <ul> <li>This tests the entire pipeline of the Smart Pixels ML project. It loads the dataset, preprocesses it, trains the model, saves the model, and evaluates the model. </li> </ul> </li> <li> <p>Usage:     <pre><code>python test_run()\n</code></pre></p> </li> <li> <p>Example:     </p> </li> </ul>"},{"location":"testing.html#functions","title":"Functions","text":"<ol> <li> <p><code>check_directories()</code></p> <ul> <li>Description:     Checks if the essential directories (<code>TEST_ROOT</code>, <code>DATA_DIR</code>, <code>LABELS_DIR</code>, <code>TFRECORDS_DIR</code>, etc.) exist. Logs an error if any directory is missing.</li> <li>Returns: <code>True</code> if all required directories exist; otherwise, logs an error and returns False.</li> </ul> </li> <li> <p><code>generate_dummy_data(num_files=NUM_DUMMY_FILES)</code></p> <ul> <li>Description: Generates dummy Parquet files for testing, including random input data and labels.</li> <li>Parameters: <code>num_files</code> (int): The number of dummy data files to generate.</li> <li>Outputs: Creates Parquet files in <code>DATA_DIR</code> and <code>LABELS_DIR</code>.</li> </ul> </li> <li> <p><code>generate_tfrecords()</code></p> <ul> <li>Description:     Initializes data generators and generates TFRecords for training and validation datasets.</li> <li>Outputs:     TFRecord files saved in <code>TFRECORDS_DIR_TRAIN</code> and <code>TFRECORDS_DIR_VALIDATION</code>.</li> </ul> </li> <li> <p><code>load_tfrecords()</code>:</p> <ul> <li>Description:     Loads pre-generated TFRecords for training and validation datasets.</li> <li>Returns:     <code>training_generator</code> and <code>validation_generator</code>.</li> </ul> </li> <li> <p><code>test_model_generation()</code></p> <ul> <li>Description:     Builds and compiles a test model using the CreateModel function.</li> <li>Returns:      A compiled Keras model.</li> <li>Raises:     Logs an error and exits if the model cannot be built.</li> </ul> </li> <li> <p><code>test_train_model()</code></p> <ul> <li>Description:     Tests model training by generating dummy data, creating TFRecords, and training the model.</li> <li>Outputs:     Logs training progress, final validation loss, and saves model weights during training.</li> </ul> </li> <li> <p><code>run_smoke_test()</code>:</p> <ul> <li>Description:     Runs the entire smoke test, including data generation, model training, and evaluation.</li> <li>Outputs:     Logs the final evaluation metrics and saves the model.</li> </ul> </li> </ol>"},{"location":"testing.html#some_testpy","title":"<code>some_test.py</code>","text":""},{"location":"testing.html#functions_1","title":"Functions","text":"<p>...</p>"},{"location":"testing.html#todo","title":"TODO \ud83d\udcdd","text":"<ul> <li>\ud83d\udd04 Add tests for:<ul> <li>Loading models from various formats (.hdf5, .h5, .pb, etc.).</li> <li>Plotting training/evaluation results.</li> <li>Evaluating metrics like accuracy, loss, etc.</li> </ul> </li> <li>\ud83d\ude80 Implement benchmarks tests for:<ul> <li>Model training and evaluation speed.</li> <li>Memory usage.</li> <li>GPU utilization.</li> </ul> </li> </ul>"},{"location":"usage.html","title":"Usage Guide","text":"<p>This is a guide for using the Smart Pixels ML project to train and evaluate the models on the simulated data.</p>"},{"location":"usage.html#1-installation","title":"1. Installation","text":"<p>Refer to Readme for installation instructions.</p>"},{"location":"usage.html#2-data-collection","title":"2. Data Collection","text":"<p>Download the simulated data from zenodo and PixelAV Add other links here</p> <p>Ensure the two directories Data and Labels are present.</p>"},{"location":"usage.html#3-data-preparation","title":"3. Data Preparation","text":"<ul> <li>Define the paths to the data and labels directories (look at utils for more details)</li> <li>Configure datagenerator parameters (look at data_generator for more details)</li> <li>Create training and validation datagenerators</li> </ul>"},{"location":"usage.html#4-model-creation","title":"4. Model Creation","text":"<ul> <li>Define the model architecture and compile.</li> <li>Also look at the summary of the model to ensure it is correct. Look at model for more details on how to do that. For loss function see loss.</li> </ul>"},{"location":"usage.html#5-model-training","title":"5. Model Training","text":"<p>If everything is set up correctly, the training should start and run seamlessly. For example: <pre><code>model.fit(\n    x=training_generator, \n    validation_data=validation_generator, \n    epochs=200, \n    verbose=1)\n</code></pre></p> <p>After training, check the loss and accuracy of the model. And save the model weights.</p>"},{"location":"usage.html#6-model-evaluation","title":"6. Model Evaluation","text":"<p>Initiate the model and Load the weights as <pre><code>model=CreateModel((13,21,2),n_filters=5,pool_size=3)\nmodel.load_weights(\"model_weights.h5\")\n</code></pre> And then evaluate the model as done in evaluate.py or in evaluate.py</p>"},{"location":"usage.html#7-model-prediction","title":"7. Model Prediction","text":"<p>Look at predict</p>"},{"location":"usage.html#8-add-additional-instructions","title":"8. Add Additional Instructions","text":"<p>Here are some additional instructions</p>"},{"location":"api/data_generator.html","title":"datagenerator module","text":"<p>This module contains the <code>OptimizedDataGenerator</code> class, which generates batches of data for training and validation during model training. This datagenerator handles the loading and processing of the data, including shuffling, standardization, and quantization of the data. It does by pre-processing the data and saving it as TFRecord files and then loading the batches on the fly during training.</p>"},{"location":"api/data_generator.html#methods","title":"Methods","text":""},{"location":"api/data_generator.html#__init__","title":"<code>__init__(...)</code>","text":"<p>Initialize the <code>OptimizedDataGenerator</code> class with the specified parameters to configure the data generator for preprocessing and batching.</p>"},{"location":"api/data_generator.html#arguments","title":"Arguments","text":"<p>Described in the comments of the <code>__init__</code> method of the OptimizedDataGenerator.py file.</p>"},{"location":"api/data_generator.html#example-usage","title":"Example Usage","text":""},{"location":"api/data_generator.html#initializing-the-data-generators","title":"Initializing the Data Generators","text":"<pre><code>training_generator = OptimizedDataGenerator(\n    data_directory_path = \"path/to/data/\",\n    labels_directory_path = \"path/to/labels/\",\n    is_directory_recursive = False,\n    file_type = \"parquet\",\n    data_format = \"3D\",\n    batch_size = val_batch_size,\n    file_count = val_file_size,\n    to_standardize= True,\n    include_y_local= False,\n    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n    input_shape = (2,13,21), # (20,13,21),\n    transpose = (0,2,3,1),\n    shuffle = False, \n    files_from_end=True,\n\n    tfrecords_dir = \"path/to/tfrecords/\",\n    use_time_stamps = [0, 19], #-1\n    max_workers = 1, # Don't make this too large (will use up all RAM)\n    seed = 10, \n    quantize = True # Quantization ON\n)\n</code></pre>"},{"location":"api/data_generator.html#loading-the-data-generators","title":"Loading the Data Generators","text":"<p>Already generated TFRecords can be reused by setting <code>load_from_tfrecords_dir</code> as <pre><code>training_generator = OptimizedDataGenerator(\n    load_from_tfrecords_dir = \"path/to/tfrecords/\",\n    shuffle = True,\n    seed = 13,\n    quantize = True\n)\n</code></pre></p> <p>The same goes for the <code>validation generator</code>. </p>"},{"location":"api/data_generator.html#using-the-data-generators","title":"Using the Data Generators","text":"<p>The data generators can be directly passed to the fit method of a Keras model.</p> <pre><code>history = model.fit(\n                        x=training_generator,\n                        validation_data=validation_generator,\n                        #callbacks=[es, mcp, csv_logger],\n                        epochs=1000,\n                        shuffle=False,\n                        verbose=1\n )\n</code></pre>"},{"location":"api/evaluate.html","title":"Evaluate Module","text":"<p>The <code>evaluate.py</code> file defines the evaluation function for the model. </p>"},{"location":"api/evaluate.html#functions-evaluateconfig","title":"Functions: <code>evaluate(config)</code>","text":"<ul> <li>Arguments:<ul> <li><code>config</code> (dict): Configuration dictionary containing the parameters for evaluation.</li> </ul> </li> </ul>"},{"location":"api/evaluate.html#example-usage","title":"Example Usage:","text":"<pre><code>config = {\n    \"weightsPath\": \"path/to/weights.hdf5\",\n    \"outFileName\": \"path/to/evaluation_results.csv\",\n    \"data_directory_path\": \"path/to/data/\",\n    \"labels_directory_path\": \"path/to/labels/\",\n    \"n_filters\": 5,\n    \"pool_size\": 3,\n    \"val_batch_size\": 500,\n    \"val_file_size\": 10\n}\nevaluate(config)\n</code></pre>"},{"location":"api/loss.html","title":"Loss Module","text":"<p>This module uses a Mixture Density Network(MDN) and hence a negative log-likelihood loss function. It uses TensorFlow and TensorFlow Probability for the loss computation.</p>"},{"location":"api/loss.html#loss-function","title":"Loss Function","text":""},{"location":"api/loss.html#custom_lossy-p_base-minval1e-9-maxval1e9-scale512","title":"<code>custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale=512)</code>","text":"<p>Calculates the Negative Log-Likelihood (NLL) for a batch of data using the model's predicted parameters.</p>"},{"location":"api/loss.html#brief-description","title":"Brief Description","text":"<p>The model parameters are the mean and the lower triangular part of the covariance matrix.</p> <p>loss function is vectorized with batches.</p>"},{"location":"api/loss.html#arguments","title":"Arguments","text":"<ul> <li><code>y</code> (tf.Tensor): The target data, shape (batch_size, 4).</li> <li><code>p_base</code> (tf.Tensor): The predicted parameters, shape (batch_size, 16).</li> <li><code>minval</code> (float): The minimum value for the likelihood, default 1e-9.</li> <li><code>maxval</code> (float): The maximum value for the likelihood, default 1e9.</li> <li><code>scale</code> (float): .</li> </ul>"},{"location":"api/loss.html#returns","title":"Returns","text":"<ul> <li><code>tf.Tensor</code>: Negative Log-Likelihood (NLL) for the given batch., shape (batch_size,).</li> </ul>"},{"location":"api/loss.html#example-usage","title":"Example Usage","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom loss import custom_loss\nfrom models import CreateModel\n\nmodel = CreateModel((13, 21, 2), n_filters=5, pool_size=3)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)\n</code></pre>"},{"location":"api/models.html","title":"Models Module","text":"<p>The <code>models.py</code> file defines Mixture Density Network (MDN) network with a 4D Multivariate Normal Distribution neural network architecture using quantized layers. The implementation uses QKeras to quantize the weights and activations of the network.</p>"},{"location":"api/models.html#functions","title":"Functions","text":""},{"location":"api/models.html#createmodelshape-n_filters-pool_size","title":"<code>CreateModel(shape, n_filters, pool_size)</code>","text":"<p>Creates a quantized neural network model for regression task with quantized layers and activations as in Model. The model has <code>14</code> output nodes with <code>4</code> being the target variables and the rest <code>10</code> being the co-variances.</p> <ul> <li>Arguments:</li> <li><code>shape</code> (tuple): Input shape (e.g., <code>(13, 21, 2)</code>/ <code>(13, 21, 20)</code>).</li> <li><code>n_filters</code> (int): Number of filters for the convolutional layers.</li> <li><code>pool_size</code> (int): Size of the pool for the pooling layer.</li> <li>Returns:</li> <li><code>keras.Model</code>: A compiled Keras model instance.</li> <li>Example:  ```python   from models import CreateModel</li> </ul> <p>model = CreateModel((13, 21, 2), n_filters=5, pool_size=3)   model.summary()</p>"},{"location":"api/models.html#additional-helper-functions","title":"Additional Helper Functions","text":""},{"location":"api/models.html#conv_networkvar-n_filters5-kernel_size3","title":"<code>conv_network(var, n_filters=5, kernel_size=3)</code>","text":"<p>Defines the convolutional network block, with quantized layers and activations.</p> <ul> <li>Arguments:</li> <li><code>var (InputLayer: tf.Tensor)</code>: Input tensor.</li> <li><code>n_filters (int)</code>: Number of filters.</li> <li> <p><code>kernel_size (int)</code>: Kernel size.</p> </li> <li> <p>Returns:</p> </li> <li><code>tf.Tensor</code>: Output tensor.</li> </ul>"},{"location":"api/models.html#var_networkvar-hidden10-output2","title":"<code>var_network(var, hidden=10, output=2)</code>","text":"<p>Defines the dense network block, with quantized layers and activations.</p> <ul> <li>Arguments:</li> <li><code>var (InputLayer: tf.Tensor)</code>: Input tensor.</li> <li><code>hidden (int)</code>: Number of hidden units.</li> <li> <p><code>output (int)</code>: Number of output units.</p> </li> <li> <p>Returns:</p> </li> <li><code>tf.Tensor</code>: Output tensor.</li> </ul>"},{"location":"api/plotting.html","title":"Plot Module","text":"<p>Add Plot results here</p>"},{"location":"api/utils.html","title":"Utils Module","text":"<p>This module contains utility functions to manage file operations and GPU configurations.</p>"},{"location":"api/utils.html#functions","title":"Functions","text":""},{"location":"api/utils.html#safe_remove_directorydirectory_path","title":"<code>safe_remove_directory(directory_path)</code>","text":"<p>Safely removes a directory if it exists.</p> <ul> <li>Arguments:</li> <li><code>directory_path</code> (str): Path to the directory to be removed.</li> <li>Example:   ```python   from utils import safe_remove_directory   safe_remove_directory(\"./temp_folder\")</li> </ul>"},{"location":"api/utils.html#check_gpu","title":"<code>check_GPU()</code>","text":"<p>Checks for available GPUs and sets memory growth to prevent allocation issues.</p> <ul> <li>Arguments:</li> <li>None.</li> <li>Return</li> <li>Prints GPU information.</li> <li>Example:   ```python   from utils import safe_remove_directory   safe_remove_directory(\"./temp_folder\")</li> </ul>"}]}